{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ã…F_TextGen2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJYqj4KX6Du5",
        "colab_type": "text"
      },
      "source": [
        "# Text Generation pt 2\n",
        "\n",
        "In our second part of Text Generation we'll move from statistical methods to neural networks. However, for the willing one it could be fun to take the last session and apply different improvements (such as back-off).  \n",
        "\n",
        "Remember how it worked? We grabbed ngrams of either characters or words, saved statistics on how often word(s) followed another word. Basically:  \n",
        "\"`Hello you poor old bastard. Hello you damn soul. Hello there.`\"  \n",
        "Yields the most probable follow-up to \"`Hello`\" to be `you`, \"`there`\" is a bit more random.\n",
        "\n",
        "\n",
        "## Neural Text Generation\n",
        "For this session we're moving on to Neural Networks, more exactly a Sequence-to-Sequence structure. I know some of you have already actually generated text through this technique, remember? Earlier we translated English to Swedish.\n",
        "\n",
        "We'll use Tensorflow 2.0 for this workshop, mainly because I've a tiny bit more experience in TF rather than PyTorch.\n",
        "\n",
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zh5JcZakH1m8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWwFIflcH7lZ",
        "colab_type": "text"
      },
      "source": [
        "### Download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-yVQxnEH9ah",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUy6bUXoH_Rq",
        "colab_type": "text"
      },
      "source": [
        "### And a little data understanding\n",
        "As often said, it's important to understand & know your data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqvTEm6LIHSg",
        "colab_type": "code",
        "outputId": "097b9e16-2efa-4853-de25-5f32f1381911",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Read the whole blob to include newlines.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# As we're gonna create a char2char network the length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxqeyLp-IJz6",
        "colab_type": "code",
        "outputId": "2b47eea2-1e68-4a75-ff1d-0cf84d4e2f69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2TF1wDLIZBl",
        "colab_type": "code",
        "outputId": "0c173112-8882-47a5-c730-18f7544ba5ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# The unique characters (classes) in the file\n",
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFGXKiGvIbuJ",
        "colab_type": "text"
      },
      "source": [
        "### Process text\n",
        "#### Vectorizing\n",
        "We can't process strings but need to convert them into an numerical representation. Create two lookup tables, one for each mapping (char2idx, idx2char)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNLXqNcFt_bm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = # TODO\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkkFDl4zuBcC",
        "colab_type": "text"
      },
      "source": [
        "Now we have mapped each unique word into an integer (0..len(vocab))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8tZBSH12Mim",
        "colab_type": "code",
        "outputId": "e6cd6e60-6ab4-4e4f-a22a-1a40bbcb3a61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        }
      },
      "source": [
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  ' ' :   1,\n",
            "  '!' :   2,\n",
            "  '$' :   3,\n",
            "  '&' :   4,\n",
            "  \"'\" :   5,\n",
            "  ',' :   6,\n",
            "  '-' :   7,\n",
            "  '.' :   8,\n",
            "  '3' :   9,\n",
            "  ':' :  10,\n",
            "  ';' :  11,\n",
            "  '?' :  12,\n",
            "  'A' :  13,\n",
            "  'B' :  14,\n",
            "  'C' :  15,\n",
            "  'D' :  16,\n",
            "  'E' :  17,\n",
            "  'F' :  18,\n",
            "  'G' :  19,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7IUcWDC2PaL",
        "colab_type": "text"
      },
      "source": [
        "And vectorizing a sentence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4c0nSHG2RVV",
        "colab_type": "code",
        "outputId": "0ab53835-d9e8-46e2-cdc5-e81f0ddc03ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Show how the first 13 characters from the text are mapped to integers\n",
        "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen' ---- characters mapped to int ---- > [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIi3HQG22T0B",
        "colab_type": "text"
      },
      "source": [
        "### The prediction task\n",
        "As in the last session we want to predict the next character given the previous character(s).  \n",
        "To visualize this, we have a sequence of characters, and we will predict the next one one time-step at a time, that is one character at a time.\n",
        "\n",
        "Recurrent Neural Networks (RNN) are very well suited for this task. RNNs maintain an interanl staet that depends on the previously seen elements, so we'll predict the next character basically given by all characters computed this far.  \n",
        "\n",
        "Improvements for future: \n",
        "\n",
        "1.   Bidirectional RNNs (why?)\n",
        "2.   Attention (why?)\n",
        "3.   Transformers\n",
        "\n",
        "#### Create training & target examples\n",
        "Even though we have \"infinite\" sequences they're not infinite, we need to chop them up. So next up is to divide the sequences into `seq_length`.  \n",
        "\n",
        "For each input sequence the target will contain `seq_length` characters as well but shifted one character to the right.\n",
        "\n",
        "Example:  \n",
        "`seq_length=3` and text = \"Hurray\", `input=\"Hur\"` and `target=urr`.\n",
        "\n",
        "To do this we can use `tf.data.Dataset.from_tensor_slices` function to convert the text vector into a stream of character indices.\n",
        "\n",
        "So break the text into chunks of seq_length+1. For example, say seq_length is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NjFPVWn4l_3",
        "colab_type": "code",
        "outputId": "c38b384a-b501-45d5-9641-fb58d8ddf626",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        " \n",
        "# use tf.data.Dataset.from_tensor_slices\n",
        "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices\n",
        "char_dataset = # TODO\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMosRZJ44tkA",
        "colab_type": "text"
      },
      "source": [
        "Tensorflow provides another useful function `batch`. `batch` lets us convert these individual characters to sequences of the desired size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x1_6iOM42NN",
        "colab_type": "code",
        "outputId": "a917a32f-bc10-4ec5-c567-3ab4444510e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "# Create sequences through batch. Datasets contains this method, dont forget length. And apply drop_remainder.\n",
        "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch\n",
        "sequences = # TODO\n",
        "\n",
        "# Visualization\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pob74v1h5UgU",
        "colab_type": "text"
      },
      "source": [
        "Let's create the targets.  \n",
        "For every sequence, shift and duplicate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rs2gcBGp5sWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = # TODO\n",
        "    target_text = # TODO\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lny1h92j52fB",
        "colab_type": "text"
      },
      "source": [
        "As mentioned each character in the sequence is a time-step. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yZVwsFl6AUm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HovOwIpm6BOD",
        "colab_type": "text"
      },
      "source": [
        "#### Create training batches\n",
        "First, what is a batch again? Anyone recall?\n",
        "\n",
        "After each batch has been predicted we update weights through backpropagation. That means, we basically learn nothing in-between. If we'd stop in the middle of a batch the earlier samples wouldn't help us out for future predictions. \n",
        "\n",
        "Why don't we just train on all data at the same time?  \n",
        "That's because of memory, and speed.\n",
        "\n",
        "Why don't we just set batch-size to 1?  \n",
        "The smaller the batch, the less accurate the estimate of the gradient is (we need statistics, right?). See this image where green is mini-batch:\n",
        "![alt text](https://i.stack.imgur.com/lU3sx.png) (StackOverflow)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCGfNx4J7cU-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJhXSXJH7gVn",
        "colab_type": "text"
      },
      "source": [
        "#### Build the model\n",
        "![alt text](https://media2.giphy.com/media/S5fMUcgKUs04E/giphy.webp?cid=790b7611db0c8b5845b9b3ce4659fcf0a15f0ae4753e2a81&rid=giphy.webp)\n",
        "\n",
        "To simplify our life we'll use Keras in Tensorflow 2.0. Keras, as mentioned, is an abstraction which helps us to focus on the big part. Pure tensorflow-code let's us dive into the small parts and optimize the final parts. As we're focusing on getting things done we'll not do that, but for companies that could be the deciding factor. Improving scores by using a custom loss-function can help as an example. Or transforming data manually between layers. All this is harder in Keras.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2kGjR4-8L6X",
        "colab_type": "text"
      },
      "source": [
        "##### Keras\n",
        "We'll use `tf.keras.Sequential` to define our model.  \n",
        "As earlier we'll start of by embedding our text using an embedding-layer (`tf.keras.layers.Embedding`) with the input being vocab-size and embedding dim as the vector space.  \n",
        "Next up we'll put a GRU-layer (LSTM works too), `tf.keras.layers.GRU`.  \n",
        "Finally we'll have an output layer (i.e.the classifier) in the form of a Dense layer (I've never seen anything else used tbh) - `tf.keras.layers.Dense`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8Gwc9dF8KKB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_EsnicI7ftq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO fill out the layers.\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense()\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DU-J3As9YMAw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oPuSnrRZb1x",
        "colab_type": "text"
      },
      "source": [
        "Run-through for each character (timestep):\n",
        "\n",
        "\n",
        "1.   Model looks up embedding (which is trained during training)\n",
        "2.   Model runs the GRU one timestep with embedding as input\n",
        "3.   Apply dense layer to classify next character\n",
        "\n",
        "![alt text](https://www.tensorflow.org/tutorials/text/images/text_generation_training.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDr1cCAHZ100",
        "colab_type": "text"
      },
      "source": [
        "#### Trial\n",
        "Let's test this!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXV-_ztGZ7b3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFq68SPQZ0bx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Notice how above has length 100, but model really has None.\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNP2QB-SapKC",
        "colab_type": "text"
      },
      "source": [
        "To get actual predictions from the model we need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary.\n",
        "\n",
        "**OBS** it is important to _not_ use the argmax but rather sample over the distritubtion as the model otherwise easily gets stuck in a loop (I believe we noticed this once during the statistical approach too).\n",
        "\n",
        "Let's go through this step by step now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwuZ1HHZgoWr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VU3z2QEgxPv",
        "colab_type": "text"
      },
      "source": [
        "`sampled_indices` now contains the predicted now contains a prediction for each timestep in the sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQnYhuAqgqSw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMJEl-_Kg6ak",
        "colab_type": "text"
      },
      "source": [
        "Decoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkbR4fx8grY9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8j8SMBRIg9gY",
        "colab_type": "text"
      },
      "source": [
        "#### Training\n",
        "\n",
        "At this point the problem can be treated as a standard classification problem. Given the previous RNN state, and the input this time step, predict the class of the next character.\n",
        "\n",
        "##### Attach an optimizer, and a loss function\n",
        "The standard `tf.keras.losses.sparse_categorical_crossentropy` loss function works in this case because it is applied across the last dimension of the predictions.\n",
        "\n",
        "Because our model returns logits, we need to set the from_logits flag.\n",
        "\n",
        "P.S.  \n",
        "If you feel confused by the `sparsed_categorical_crossentropy`, why have `sparse`? It is because we have integers.  \n",
        "`categorical_crossentropy` is applied when we one-hot-encode our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SqixfzLhYea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlYHRMpwjT4G",
        "colab_type": "text"
      },
      "source": [
        "Personally the go-to optimizer is `Adam` but one can feel free to try out `RMSProp` or any other. Let's compile the model.\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1240/1*SjtKOauOXFVjWRR7iCtHiA.gif)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sOZ_E_qjZoi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile # TODO add compile options"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nf6Wk1XjcEI",
        "colab_type": "text"
      },
      "source": [
        "##### Checkpoints\n",
        "Checkpoints, checkpoints and even more checkpoints. I love checkpoints, and you should too.  \n",
        "We'll use a `tf.keras.callbacks.ModelCheckpoint` to make sure that checkpoints are saved during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeI5SBUejuxR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2gGo2tgjzxD",
        "colab_type": "text"
      },
      "source": [
        "##### Execution\n",
        "To keep training time reasonable, use 10 epochs to train the model. **DON'T** forget to turn on GPU, we're talking RNNs here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrC9iRyujutF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0X0zlxWRo6OG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtOTIZiH7zKO",
        "colab_type": "text"
      },
      "source": [
        "#### Text Generation\n",
        "##### Restore the last check point\n",
        "To keep this prediction step simple, use a batch size of 1.\n",
        "\n",
        "Because of the way the RNN state is passed from timestep to timestep, the model only accepts a fixed batch size once built.\n",
        "\n",
        "To run the model with a different batch_size, we need to rebuild the model and restore the weights from the checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlS3EjKw9Mgi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLWQtP-69O_A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = # TODO build model\n",
        "\n",
        "model.load_weights # TODO load weights\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRxL4yrO9UBl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lsc7XPGv9Vas",
        "colab_type": "text"
      },
      "source": [
        "##### Prediction loop\n",
        "The following code block generates the text:\n",
        "\n",
        "*   It Starts by choosing a start string, initializing the RNN state and setting the number of characters to generate.\n",
        "*   Get the prediction distribution of the next character using the start string and the RNN state.\n",
        "*   Then, use a categorical distribution to calculate the index of the predicted character. Use this predicted character as our next input to the model.\n",
        "*   The RNN state returned by the model is fed back into the model so that it now has more context, instead than only one word. After predicting the next word, the modified RNN states are again fed back into the model, which is how it learns as it gets more context from the previously predicted words.\n",
        "![alt text](https://www.tensorflow.org/tutorials/text/images/text_generation_sampling.png)\n",
        "Looking at the generated text, you'll see the model knows when to capitalize, make paragraphs and imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqmQHpSg9obY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate =\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = # TODO\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the word returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted word as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append # TODO revert idx to char\n",
        "\n",
        "  return (start_string + ''.join(text_generated))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OND5FPlZ9ptn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(generate_text(model, start_string=u\"ROMEO: \"))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1K0HQSd9rTb",
        "colab_type": "text"
      },
      "source": [
        "The easiest thing you can do to improve the results it to train it for longer (try EPOCHS=30).\n",
        "\n",
        "You can also experiment with a different start string, or try adding another RNN layer to improve the model's accuracy, or adjusting the temperature parameter to generate more or less random predictions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUSzUxVA9t2G",
        "colab_type": "text"
      },
      "source": [
        "## If time, let's do advanced\n",
        "https://www.tensorflow.org/tutorials/text/text_generation#advanced_customized_training"
      ]
    }
  ]
}