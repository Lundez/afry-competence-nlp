{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AfNLP20190204.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{},"source":["**Google Colab** deep-link [here](https://colab.research.google.com/github/afry-south/nlp-competence/blob/master/python/20190204%20-%20%231%20%5BTextClassification%5D/20190204-Text-Classification-IMDB.ipynb)"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":173},"colab_type":"code","executionInfo":{"elapsed":101655,"status":"ok","timestamp":1549307720687,"user":{"displayName":"Hampus Londögård","photoUrl":"","userId":"16122122575432769407"},"user_tz":-60},"id":"MMAGNTwTqY3I","outputId":"3d3d95f4-c095-45b1-d80e-d99595e9216b"},"outputs":[],"source":["!pip install pandas\n","!pip install urllib3\n","import urllib.request\n","from google.colab import files\n","from datetime import datetime\n","import os\n","import sklearn\n","import numpy as np\n","import tarfile\n","import pandas as pd\n","url = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n","file_name = 'aclImdb_v1.tar.gz'\n","#url2 = 'https://zenodo.org/record/49899/files/preprocessed.tar.gz?download=1'\n","#file_name2 = 'preprocess.tar.gz'\n","urllib.request.urlretrieve(url, file_name)\n","#urllib.request.urlretrieve(url2, file_name2)\n","print(\"Extracting...\")\n","tarfile.open(file_name).extractall()\n","#tarfile.open(file_name2).extractall()\n","print(\"Extraction successful!\")\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"p7mvQNcrpuuG"},"source":["# Åf Competence - NLP/ML 2019-02-04 (Hampus Londögård)\n","## Text Classification\n","Today we'll go through  text classification, what it is, how it is used and how to make it yourself while trying to keep  have a great mix of both theory and practical use.\n","Text classification is just what the name suggest, a way to classify texts. Let it be spam or reviews, you train it and it'll predict what class the text belongs to.\n","\n","---\n","\n","\n","### A good baseline\n","To have a good baseline is incredibly important in Machine Learning. In summary you want the following\n","\n","\n","*   Simple model to predict outcome\n","*   Use this model to compare your new, more complex model to\n","\n","This is to be able to know what progress you're making. You don't want to do anything more complex without any gains. \n","\n","One pretty common simple baseline is just to pick a random class as prediction. \n","\n","### Classes & Features\n","What is a class and feature?\n","\n","Features are the input to the model, you can see a machine learning system as a \"consumer\" of features. You can view this as a cookie monster consuming cookies and then he says if they taste good or bad. He has the input, cookie, that can be a feature. He then has a output, class, that is good/bad. Repeat this a lot of times and you can retrieve statistics if Cookie Y is good or bad. \n","\n","To generalize this system we would divide the feature into multiple feature, like what ingredients the cookie contains. \n","So instead of saying this is a \"Chocolate Chip Cookie\" we know tell the system the features are: \n","```\n","chocolate: yes\n","sugar:yes\n","honey:no\n","oat:no\n","cinnamon: no\n","sweet: yes\n","sour: no\"\n","```\n",". In numerical input it would translate to something as `[1,1,0,0,0,1,0]`. \n","\n","#### One-Hot-Encoding - how we represent features & classes\n","As shown in the translation to numerical vectors we don't represent words as actual words. We always use numbers, often we even use something called _One-Hot-Encoding_.\n","\n","One-Hot-Encoding means that we have an array of one 1 and the rest is 0s. This is to optimize math performed by the GPU (or CPU). \n","\n","Using the example of _Good_ & _Bad_ cookies with the extension of _Decent_ we will One-Hot-Encode these as the following\n","```\n","Good   = [1,0,0]\n","Bad    = [0,1,0]\n","Decent = [0,0,1]\n","```\n","The same is applied to our features. If you're using a framework (such as Keras) it is pretty common that they include an method to do this, or even that it is done automatically for you.\n","\n","### Back to text classification\n","To classify a text we do what is called an _sentiment analysis_ meaning that we try to estimate the *sentiment polarity* of a text body. In the first part of this workshop we'll be assuming that there's only two sentiments, _Negative_ and _Positive_. Then we can express this as the following classification problem:\n","```\n","Feature: String body\n","Class:   Bad|Good\n","```\n","The output, _Classes_, are easy to One-Hot-Encode but how do we succesfully One-Hot-Encode a string? A character can be seen as a class but is that really something we can learn from? To solve this we need to preprocess our input somehow.\n","\n","### Preprocessing\n","Preprocessing is an incredibly important part of Machine Learning. Combining preprocessing with _Data Mining_ is actually around 70% of the workload (IBM) when developing models through the CRISP-DM. From my experience this is true. \n","\n","Having good data and finding the most important features is incredibly important to have a competent system. \n","In this task we need to preprocess the text to simplify the learning process for our system. We will do the following:\n","\n","\n","*   Clean the text\n","*   Vectorize the texts into numerical vectors\n","\n","#### Cleaning the text\n","Why do we need to clean the text? It is to remove weird stuff & outliers. If we have the text `I'm a cat.` we want to simplify this into `[i'm, a, cat]` or even `[im, a, cat]`. \n","\n","Removing data such as non-alphabetical characters and the letter case makes more data look a like and reduces the dimension of our input -- this simplifies the learning of the system. But removing features can be bad also, if someone writes in all CAPS we can guess that they're angry. But let's take that later. \n"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"JTK5wyf9Ut6K"},"outputs":[],"source":["import regex as re\n","\n","\n","def clean_text(text):\n","    \"\"\"\n","    Applies some pre-processing on the given text.\n","\n","    Steps :\n","    - Removing punctuation\n","    - Lowering text\n","    \"\"\"\n","    \n","    # remove the characters [\\], ['] and [\"]\n","    text = re.sub(r\"\\\\\", \"\", text)    \n","    text = re.sub(r\"\\'\", \"\", text)    # Extra: Is regex needed? Other ways to accomplish this.\n","    text = re.sub(r\"\\\"\", \"\", text)\n","    # replace all non alphanumeric with space \n","    text = re.sub(r\"\\W+\", \" \", text)\n","    # text = re.sub(r\"<.+?>\", \" \", text) # <br></br>hej<br></br>\n","    \n","    # Extra: How would we go ahead and remove HTML? Time to learn some Regex!\n","    \n","    return text.strip().lower()"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","executionInfo":{"elapsed":403,"status":"ok","timestamp":1549298422083,"user":{"displayName":"Hampus Londögård","photoUrl":"","userId":"16122122575432769407"},"user_tz":-60},"id":"envHbFTkWfBp","outputId":"a8bfb0b4-d716-4ce6-8048-b307cc2ad7a6"},"outputs":[],"source":["clean_text(\"Wow, we can clean text now. Isn't that amazing!?\").split()"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"y-uy6SdMYQos"},"source":["#### Vectorization\n","Now that we can extract text we need to be able to input it to the system. We have to vectorize it. In this part we'll vectorize each word as a number. \n","The simplest approach to this is using *Bag of Words* (BOW).\n","\n","Bag of Words creates a list of words which is called the _Dictionary_. The Dictionary is just a list of the words from the training data.\n","```\n","Training data: [\"ÅF is a big company\", \"ÅF making future\"]\n","--> Dictionary: [ÅF, is, a, big, company, making, future]\n","\n","New text: \"ÅF company is a future company\" --> [1,1,1,0,2,0,1]\n","```\n","Our new text is vectorized on top of the dictionary. You take the dictionary and replace the words position with the count of it that is found in the new text.\n","\n","#### Finalizing the preprocessing\n","We can actually do some more things to improve the system which I won't go into detail about (read the code). We remove stop-words and so on. \n"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":80},"colab_type":"code","executionInfo":{"elapsed":608,"status":"ok","timestamp":1549298976873,"user":{"displayName":"Hampus Londögård","photoUrl":"","userId":"16122122575432769407"},"user_tz":-60},"id":"i0KVyB3i6adZ","outputId":"b0858724-add1-4b82-bff4-36ce888544d8"},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","\n","training_texts = [\n","    \"ÅF is a big company\", \n","    \"ÅF making future\"\n","]\n","\n","test_texts = [\n","    \"ÅF company is a future company\"\n","]\n","\n","# this is the vectorizer\n","vectorizer = CountVectorizer(\n","    stop_words=\"english\",    # Removes english stop words (such as 'a', 'is' and so on.)\n","    preprocessor=clean_text  # Customized preprocessor\n",")\n","\n","# fit the vectorizer on the training text\n","vectorizer.fit(training_texts)\n","\n","# get the vectorizer's vocabulary\n","inv_vocab = {v: k for k, v in vectorizer.vocabulary_.items()}\n","vocabulary = [inv_vocab[i] for i in range(len(inv_vocab))]\n","\n","# vectorization example\n","pd.DataFrame(\n","    data=vectorizer.transform(test_texts).toarray(),\n","    index=[\"Test sentence\"],\n","    columns=vocabulary\n",")\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3Yld3Sae8y_d"},"source":["\n","### Let's do something fun out of this!\n","To begin with we need data. Luckily I know a perfect dataset for this -- the IMDB movie reviews from stanford. This is a widely used dataset throughout _Sentiment Analysis_. The data contains 50 000 reviews where 50 % is positive and the rest negative. \n","First we fetch a dataset. \n","Download [this file](http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz) and unpack it (into `aclImdb`) if the first code-snippet was unsuccessful. \n"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"yTA48MntUsIS"},"outputs":[],"source":["import os\n","import numpy as np\n","\n","def load_train_test_imdb_data(data_dir):\n","    \"\"\"\n","    Loads the IMDB train/test datasets from a folder path.\n","    Input:\n","    data_dir: path to the \"aclImdb\" folder.\n","    \n","    Returns:\n","    train/test datasets as pandas dataframes.\n","    \"\"\"\n","\n","    data = {}\n","    for split in [\"train\", \"test\"]:\n","        data[split] = []\n","        for sentiment in [\"neg\", \"pos\"]:\n","            score = 1 if sentiment == \"pos\" else 0\n","\n","            path = os.path.join(data_dir, split, sentiment)\n","            file_names = os.listdir(path)\n","            for f_name in file_names:\n","                with open(os.path.join(path, f_name), \"r\") as f:\n","                    review = f.read()\n","                    data[split].append([review, score])\n","  \n","    # We shuffle the data to make sure we don't train on sorted data. This results in some bad training.\n","    np.random.shuffle(data[\"train\"])        \n","    data[\"train\"] = pd.DataFrame(data[\"train\"],\n","                                 columns=['text', 'sentiment'])\n","\n","    np.random.shuffle(data[\"test\"])\n","    data[\"test\"] = pd.DataFrame(data[\"test\"],\n","                                columns=['text', 'sentiment'])\n","\n","    return data[\"train\"], data[\"test\"]"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"Rnjg1FZf224D"},"outputs":[],"source":["train_data, test_data = load_train_test_imdb_data(\n","    data_dir=\"aclImdb/\")"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VkJAvx6a25j_"},"source":["### Let's create our classifier\n","We now have a dataset that we have successfully partitioned into a dictionary so that we can use it for our classifier. \n","\n","Do you see an issue with our baseline right now? \n","\n","...As mentioned we want to only have important features to simplify training. Right now we have an enormous amount of features, our BOW-approach result in an 80 000-dimensional vector. Because of this we _must_ use simple algorithms that learn fast & easy, e.g. [Linear SVM](https://en.wikipedia.org/wiki/Support-vector_machine), [Naive Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) or [Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression).  \n","\n","Let's create some code that actually let's us train a Linear SVM!"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":88},"colab_type":"code","executionInfo":{"elapsed":20264,"status":"ok","timestamp":1549303364499,"user":{"displayName":"Hampus Londögård","photoUrl":"","userId":"16122122575432769407"},"user_tz":-60},"id":"xcAJM5-S44MU","outputId":"db292a8c-1bf6-4514-86ab-83f50fb6b789"},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","from sklearn.svm import LinearSVC\n","\n","\n","# Transform each text into a vector of word counts\n","vectorizer = CountVectorizer(stop_words=\"english\",\n","                             preprocessor=clean_text)\n","\n","training_features = vectorizer.fit_transform(train_data[\"text\"])    \n","test_features = vectorizer.transform(test_data[\"text\"])\n","\n","# Training\n","model = LinearSVC()\n","model.fit(training_features, train_data[\"sentiment\"])\n","y_pred = model.predict(test_features)\n","\n","# Evaluation\n","acc = accuracy_score(test_data[\"sentiment\"], y_pred)\n","\n","print(\"Accuracy on the IMDB dataset: {:.2f}\".format(acc*100))"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QvUW8f8X6Un2"},"source":["### Comparison to state-of-the-art\n","Our accuracy is somewhere around 83.5-84 % which is really good! With this simple model and incredibly simplistic feature extraction we achieve a really high amount of correct answer! \n","Comparing this to state-of-the-art we're around 11 percent units beneat (~95% accuracy achieved [here](https://arxiv.org/pdf/1801.06146.pdf)).\n","\n","Incredible right? Exciting!? For me it is at least! \n","\n","How do we improve from here?\n","\n","### Improving the model\n","We have some huge improvements to make outside of fine-tuning, so we'll skip the fine-tuning from now. \n","\n","The first step is to improve our vectorization. \n","\n","#### TF-IDF\n","If you were at _first friday (@ÅF)_ you have heard about TF-IDF earlier. TF-IDF stands for _Term Frequence-Inverse Document Frequency_ and is a measurement that aims to fight imbalances in texts. \n","\n","In our vectorization step we look at the word-count meaning that we'll have some biases to how much a word is present, the longer the text the more the bias. To reduce this we can take the word-count divided by the total amount of words in the text (TF). \n","We also want to downscale the words that are incredibly frequent such as stop words and topic-related words, and upscale unusual words somewhat, e.g.  _glamorous_ might not be frequent but it is important to the text most likely. We use _IDF_ for this. We then take these two and combine.\n","\n","![alt text](https://cdn-images-1.medium.com/max/800/1*FgQgJYozG7colT9rys066w.png)\n","\n","### Implementation details\n","This is actually really easy to do as _sklearn_ already has a finished `TfIdfVectorizer` so all we have to do is to replace the `CountVectorizer`.\n","Let's see how it goes!"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","executionInfo":{"elapsed":15371,"status":"ok","timestamp":1549304157395,"user":{"displayName":"Hampus Londögård","photoUrl":"","userId":"16122122575432769407"},"user_tz":-60},"id":"rmT3zvMt8ttl","outputId":"55a968df-821e-4c63-f25f-feef3d43883e"},"outputs":[],"source":["from sklearn.svm import LinearSVC\n","from sklearn.metrics import accuracy_score\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","\n","# Transform each text into a vector of word counts\n","vectorizer = TfidfVectorizer(stop_words=\"english\",\n","                             preprocessor=clean_text)\n","\n","training_features = vectorizer.fit_transform(train_data[\"text\"])    \n","test_features = vectorizer.transform(test_data[\"text\"])\n","\n","# Training\n","model = LinearSVC()\n","model.fit(training_features, train_data[\"sentiment\"])\n","y_pred = model.predict(test_features)\n","\n","# Evaluation\n","acc = accuracy_score(test_data[\"sentiment\"], y_pred)\n","\n","print(\"Accuracy on the IMDB dataset: {:.2f}\".format(acc*100))\n","\n","# Extra: Implement our own TfIdfVectorizer."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"SlWfC0Mt82VS"},"source":["### Conclusion of TF-IDF\n","The `TfIdVectorizer` improved our scoring with 2 percent units, that's incredible for such an easy improvement! \n","\n","This for me shows how important it is to understand the data and what is important. You really need to grasp how to extract the important and what tools are available. \n","\n","But let's not stop here, lets reiterate and improve further.\n","\n","What is the next natural step? Context I believe.\n","During my master-thesis on spell correction of Street Names it was very obvious how important context is to increase the models understanding. Unfortunately we couldn't use the context of a sentence in the thesis (as of the nature of street names) but here we can!\n","\n","\n","### Use of context\n","Words by themself prove some meaning but sometimes they're used in a negated sense, e.g. _not good_. _Good_ in itself would most likely be positive but if we can get the context around the word we can be more sure about in what manner it is applied.\n","\n","We call this  _N-grams_ where N is equal to the amount of words taken into consideration for each word. Using bigrams (N=2) we get the following:\n","\n","`companies often use corporate bs => [companies, often, use, slogans, (companies, often), (often,use), (use,slogans)]`\n","\n","Sometimes you include a start & ending word so that it would be `(\\t, companies)` and `(slogans, \\r)` or such. In this case as we are not finetuning we won't go into that. We'll keep it simple.\n","\n","The all-mighty sklearn `TfIdfVectorizer` actually already have included N-gram support using the parameter `ngram_range=(1, N)`. So let's make it simple for us and make use of that!"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","executionInfo":{"elapsed":41864,"status":"ok","timestamp":1549305709688,"user":{"displayName":"Hampus Londögård","photoUrl":"","userId":"16122122575432769407"},"user_tz":-60},"id":"6lFMTir3RltS","outputId":"18128a08-cfdd-43b9-e84c-9dd4d6843920"},"outputs":[],"source":["from sklearn.svm import LinearSVC\n","from sklearn.metrics import accuracy_score\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","\n","# Transform each text into a vector of word counts\n","vectorizer = TfidfVectorizer(ngram_range=(1, 2),\n","                            strip_accents='ascii',\n","                            max_df=0.98)\n","\n","training_features = vectorizer.fit_transform(train_data[\"text\"])    \n","test_features = vectorizer.transform(test_data[\"text\"])\n","\n","# Training\n","model = LinearSVC()\n","model.fit(training_features, train_data[\"sentiment\"])\n","y_pred = model.predict(test_features)\n","\n","# Evaluation\n","acc = accuracy_score(test_data[\"sentiment\"], y_pred)\n","\n","print(\"Accuracy on the IMDB dataset: {:.2f}\".format(acc*100))"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"tEtvkFjPlxFx"},"source":["### Conclusion of N-gram\n","Once again we see a massive improvement. We're almost touching 89 % now! That's just a mere 6 percent units below state-of-the-art. What can we do to improve now? \n","\n","Some possible improvements for you to try!\n","*   Use a custom threshold to reduce the dimensions\n","*   Play around with the `ngram_range` (don't forget a threshold if you do this)\n","*   Improve the preprocessing\n"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"1XdJ5M5apxuW"},"outputs":[],"source":["# Try some fun things here if you want too :)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"hk-WuoUYr0xg"},"source":["## Conclusion of phase 1\n","We have created a strong baseline for text classification with great accuracy for its simplicity.\n","The following steps has been done\n","\n","\n","*   First a simple preprocessing step which is of great importance. We have to remember to not make it to complex, the complexity of preprocessing is like an evil circle in the end. In our case we remove punctuations, stopwords and lower the case.\n","*   Secondly we vectorize the data to make it readable by the system. A classifier requires numerical features. For this we had a `TfIdfVectorizer` that computes frequency of words while downsampling words that are to common & upsampling unusual words. \n","*   Finally we added N-gram to the model to increase the understanding of the sentence by supplying context. \n","\n","## Phase 2\n","How do we improve from here?\n","TF-IDF has its cons and pros. Some of the cons are that they:\n","\n","\n","*   Don't account for any kind of positioning at all\n","*   The dimensions are ridiculous large\n","*   They can't capture semantics.\n","\n","Improvements upon this is made by using neural networks and word embeddings. \n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4TAYiZnU9FZu"},"source":["## Word Embeddings\n","Word Embeddings & Neural Networks are where we left off. By change our model to instead utilize these two concepts we can improve the accuracy once again. \n","\n","### Word Embeddings\n","Word Embeddings (WE) are actually a type of Neural Network. It uses _embedding_ to create the model. \n","I quickly explained WE during my presentation on Summarization and how to build a great summarizer. Today we'll go a little more into depth. \n","\n","To begin with I'll take the most common example, WE lets us do the following arithmetiric with words: \n","\n","```\n","King - Man + Woman = Queen\n","```\n","This is, in my opinion, completely amazing and fascinating. How does this work? Where do I learn more? Those are my first thoughts. \n","In fact the theory is pretty basic until you get to the nittygritty details, as with most things. \n","\n","WE is built on the concept ot learn how words are related to eachother. What company do a word have? To make the example more complex we can redefine this too the following: `A is to B what C is to D`. \n","\n","Currently there is three \"big\" models that are widely used. The first one Word2Vec ([Mikolov et al 2013](https://arxiv.org/abs/1301.3781)), the second is GloVe (MIT [MIT](https://nlp.stanford.edu/projects/glove/), [Pennington et al 2014](https://nlp.stanford.edu/pubs/glove.pdf)) and the final one is fastText ([facebook](https://github.com/facebookresearch/fastText)).\n","\n","We will look into how you can achieve this without Deep Learning / Neural Networks unlike the models mentioned.\n","\n","#### Step 1: How to represent words in a numerical vector\n","The first thing we have to do to actually understand/achieve word embeddings is to represent words in a numerical vector. In relation to this a quick explanation of sparse & dense representations would be great. Read more in detail at [Wikipedia: Sparse Matrix](https://en.wikipedia.org/wiki/Sparse_matrix)\n","\n","**Sparse representation** is when we represent something very sparsely. It tells us that the points in the space is very few in regards to the dimensions and that most elements are empty. Think one-hot-encoding.\n","\n","A **Dense representation** in comparison has few dimensions in comparison to possible values and most elements are filled. Think of something continuous.\n","\n","The most simple way to represent words in a numerical vector is something we touched earlier, by one-hot-encoding them, i.e. a sparse representation.\n","\n","![Source :(Marco Bonzanini, 2017)](https://cdn-images-1.medium.com/max/1200/1*YEJf9BQQh0ma1ECs6x_7yQ.png)\n","(Source: Marco Bonzanini, 2017)\n","\n","Because of how languages are structured having one-hot-encoding means that we will have an incredibly sparse matrix (can be good) but it will have an enormous amount of dimensions (bad). \n","\n","On top of this how would we go ahead and measure the distance between words? Normally one would use the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) but if we have a one-hot-encoding all the words would be orthogonal against eachother meaning that the dot-product will be zero.\n","\n","Creating a dense representation however would indeed capture similarity as we could make use of cosine-similarity and more. Introducing Word2Vec.\n","#### Step 2: Word2Vec, representing data densely\n","The goal of Word2Vec, at least to my understanding, is to actually predict the context of a word. Or in other words we learn embeddings by prediciting the context of the word. The _context_ here being the same definition as in N-grams. Word2Vec uses _shallow neural network_ to learn word vectors so that each word is good at predicting its own contexts (more about his in **Skip-Grams**) and how to predict a word given a context (more about this in **CBOW**). \n","\n","#### Skip-gram\n","Skip-gram very simplified is when you train on the N-grams but without the real word. \n","![alt text](https://cdn-images-1.medium.com/max/800/1*swlaqv7p_3xI4eL37C1pAA.png)\n","\n","\n","As of now we have empirical results showing how this technique is very successful at learning the meaning of the words. On top of this the embedding that we get has both _direction of semantic and syntatic  meaning_ that are exposed in example such as `King - Man...`.\n","\n","Another example would be:\n","`Vector(Madrid) - Vector(Spain) + Vector(Sweden) ~ Vector(Stockholm)`\n","\n","#### So how do the arithmetic of words actually work?\n","I won't go into details (some complicated math, see [Gittens et al](http://www.aclweb.org/anthology/P17-1007)) but if we assume the following to be true:\n","\n","\n","*   All words are distributed uniformly\n","*   The embedding model is linear\n","*   The conditional distributions of words are indepedent\n","\n","Then we can prove that the embedding of the paraphrase of a set of words is obtained by taking the sum over the embeddings of all of the individual words.\n","\n","Using this result it's easy to show how the famous man-woman, king-queen relationship works. \n","\n","Extra note: You can show this then by havingn `King` and `Queen` having the same `Male-Female` relationship as the `King` then is the paraphrase of the set of words `{Queen, X}`\n","\n","I want to note that these assumptions are not 100 percent accurate. In reality word distributions are thought to follow Zipf's law. \n","\n","#### GloVe\n","A year after Word2Vec was a fact to the world the scientist decided to reiterate again. This time we got GloVe. GloVe tried to improve upon Word2Vec by that given a word its relationship(s) can be recovered from co-occurence statistics of a large corpus. GloVe is expensive and memory hungry, but it's only one load so the issue isn't that big.\n","Nitty bitty details\n","\n","#### fastText\n","With fastText one of the biggest problems is solved, both GloVe and Word2Vec only learn embeddings of word of the vocabulary. Because of this we can't find an embedding for a word that isn't in the dictionary. \n","\n","Bojanowski et al solved this by learning the word embeddings using subword information. \n","To summarize fastText learns embeddings of character n-grams instead. \n","\n","#### The simple way\n","A simple approach to create your own word embeddings without a neural network is by factorizing a co-occurence matrix using SVD (singular-value-decomposition). As mentioned Word2Vec is barely a neural network as it has no hidden layers nor an y non-linearities. GloVe factorizes a co-occurense matrix while gaining even better results. \n","\n","So how do we go ahead?\n","\n","We go to this blog and look into it!\n","https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/ by Stitch Fix. \n","\n","What do we do with this?\n","\n","### Conclusions"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":170},"colab_type":"code","executionInfo":{"elapsed":975,"status":"ok","timestamp":1549308339287,"user":{"displayName":"Hampus Londögård","photoUrl":"","userId":"16122122575432769407"},"user_tz":-60},"id":"Oz_zmodD1vEV","outputId":"48c2c2da-24a0-4834-9a76-a3ef4e68d644"},"outputs":[],"source":["def get_unigram_prb(text):\n","  word_prb = {}\n","  # Add all to dict. \n","  # Divide all values by total value\n","\n","def get_skipgram_prb(text):\n","  # Add word_i-1, word_i+1 to dict (to value)\n","  # Divide by all skip-grams\n","  "]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"LgOPpTfy7_xD"},"outputs":[],"source":["# Transform each text into a vector of word counts\n","# WordEmbeddings to find centroid of review\n","# Do this only. \n","# 1. Preprocess\n","# 2. Create wordEmbeddings for sentences\n","# 3. Try to combine with a bigram setup see if improvement\n","# 4. and so on..."]}]}