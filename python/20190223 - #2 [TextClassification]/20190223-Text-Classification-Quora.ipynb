{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Meeting 2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"uX8yNqyyyPb8","colab_type":"text"},"source":["# Text Classification 2\n","Today we're yet again working on text classification, we're gonna focus on [Quora Insincere Questions Classification\n","](https://www.kaggle.com/c/quora-insincere-questions-classification).\n","\n","Last meeting we went through the basics to create baseline when classifying IMDB reviews. \n","\n","### Approaches for today\n","\n","\n","1.   Transfer learning through means such as \n","> [ULMFit](https://docs.fast.ai/text.html)... [theory](https://arxiv.org/abs/1801.06146)\n","\n",">> [BERT/GPT2](https://github.com/huggingface)\n","\n",">> AllenNLP ([how to](http://www.realworldnlpbook.com/blog/training-sentiment-analyzer-using-allennlp.html))\n","\n","2.   Reapplying the same approach as last time\n","\n","3.   NO Creating our own RNN/CNN (QRNN -- potential speed-up) solution through word embeddings (GloVe/fastText/word2vec) as base (this is also a type of Transfer Learning)  [an interesting example](https://www.aaai.org/Papers/AAAI/2019/AAAI-SachanD.7236.pdf)\n","\n","4.   Using tools such as [fastText](https://fasttext.cc/docs/en/supervised-tutorial.html) and optimizing it, [theory](https://arxiv.org/pdf/1607.01759.pdf)\n","\n","5.   Simpler models such as Random Forest/XGBoost etc\n","\n","6.   Use [Flair](https://github.com/zalandoresearch/flair)... [example](https://towardsdatascience.com/text-classification-with-state-of-the-art-nlp-library-flair-b541d7add21f) \n","\n","8.   Use [SpaCy](https://spacy.io)\n","\n","[Comparisons for funs](https://arxiv.org/pdf/1801.06261.pdf) \n","\n","Great how to use [AllenNLP](https://github.com/allenai/writing-code-for-nlp-research-emnlp2018/blob/master/writing_code_for_nlp_research.pdf)\n","\n","Quora Contesters\n","\n","https://github.com/KevinLiao159/Quora\n","\n","Great read also: https://mlwhiz.com/blog/2018/12/17/text_classification/\n"]},{"cell_type":"markdown","metadata":{"id":"IQ0_ZZI-jjcA","colab_type":"text"},"source":["### Transfer Learning 101\n","Transfer Learning has become a huge thing within the ML world. I believe the first \"kick-off\" was through ImageNet which skyrocketed the applications of Image Classification.\n","\n","The basic idea is that if we've learned how to solve a problem with a lot of data we should be able to solve a pretty similar problem with little labeled data while still remaining a high accuracy by using the earlier model as a starting ground\n","\n","![alt text](https://api.ning.com/files/1a5R6o7JsEHZ9j2SOd20XYu2GYExArt4Kr*0U07Z1JYbfSnF2ugTP7wmqMJn-l2auLHblJkG2QbtZcVqzScB81vPibkAjqBg/transferlearning.png)\n","\n","In Image Classification we've ImageNet, and just 2018 (or maybe 2017, don't quote me on this) this is actually starting to get realized within the field of Natural Language Processing. The reason as of why this hasn't started directly after ImageNet is that NLP is a 'harder' area and problem to solve as sentences vary in length and we've an infinite amount of word-classes (almost). \n","\n","The early way to use a minor version of Transfer Learning is via Word Embeddings and this has been available since 2013 but starting now we see huge models that've been trained to learn a great Language Model that can be applied to a varierity of problems. \n","\n","These are ELMO, ULMFit, BERT & GPT2 (not released through OpenAI but HuggingFace are awesome and have released a trained model!).\n","\n","### Language Model\n","So what is this language model that I just mentioned? The Language Model (LM) is actually the core of most problems within NLP. It means the understanding of language. Most problems are able to be solved in a similar fashion using a LM, by reading a sentence we want to complete it somehow - be it an answer, classification, generation of more text or even summarization. We need to somehow be able to map what we get in to an output. This in its core is a language model in my opinion. \n","\n","[Wikipedia](https://en.wikipedia.org/wiki/Language_model) use the precise definition of just giving a certain sentence an probability depending on the words in it. \n","\n","### Frameworks\n","To prototype and to quickly reiterate a great framework must be used.\n","Right now there's a few different that I believe worthy noting:\n","\n","\n","*   DeepLearning4J (Java/Kotlin/Scala)\n","*   Keras (python)\n","*   PyTorch\n","*   AllenNLP (python, built on PyTorch)\n","*   SpaCy\n","*   And some extras such as ULMFit through fast.ai etc\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"_advYpE5zN89","colab_type":"code","colab":{}},"source":["\n"],"execution_count":0,"outputs":[]}]}