{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextGeneration1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHgMdCRzcMyv",
        "colab_type": "text"
      },
      "source": [
        "## Welcome to Ã…F Competence Evening (Machine Learning, NLP)\n",
        "This is part one of a four, or five, part series where we'll go through **text generation**.\n",
        "\n",
        "\n",
        "### Text Generation\n",
        "What to do is self-saying but what we need is some kind of *Language Model*. \n",
        "\n",
        "#### Language Model\n",
        "Language Models as the most simple definition is the probability of a sequence of word as whole. \n",
        "\n",
        "Think of this as `ello`, what does this move towards? `hello` or should we continue and build `mellow` perhaps? The human brain is really good at understanding the context and filling in the blanks. Depending on the \"history\" we have it is easier or harder to guess. The same applies if we use Maximum Likelihood. \n",
        "\n",
        "We can also apply this on a word-level meaning that if we have \"*How are you WORD*\" we would most likely guess \"*WORD*\" to be \"*doing*\".\n",
        "\n",
        "The conclusion is that we need to count N-grams & produce statistics out of these using Markov Chains or something like it. Looking at this we can find the following;  \n",
        "Bigram-model: $p(w) = \\prod_{i=1}^{k+1} p(w_i|w_{i-1})$  \n",
        "\n",
        "To find the probabilities given history we need to find the possibilites given the history,\n",
        "\n",
        "Estimate probabilities: $p(w_i|w_{i-1})=\\frac{c(w_{i-1}w_i)}{c(w_{i-1})}$\n",
        "\n",
        "We can expand this concept to apply to N-grams too. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHnr09HrBC83",
        "colab_type": "text"
      },
      "source": [
        "First we import the needed modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pM3-8OBZAazw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import *\n",
        "from random import random\n",
        "import string\n",
        "import numpy as np\n",
        "import spacy\n",
        "import pandas as pd\n",
        "\n",
        "nlp = spacy.load('en')\n",
        "nlp.max_length=5576562\n",
        "PADDING = \"~\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgmRVERCFTVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Can be exchange for other inputs.\n",
        "# e.g. https://github.com/ashwinmj/word-prediction/blob/master/eminem_songs_lyrics.txt\n",
        "!wget http://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUDGlObNGbO5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(open('shakespeare_input.txt', 'r').read()[:250])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QE8baU4Gbnf4",
        "colab_type": "text"
      },
      "source": [
        "### Training\n",
        "We first define training. We want to read a file & have a certain length of \"memory\" ($n$).  \n",
        "We'll start with order (memory) = $2$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "js48gl4G36Mt",
        "colab_type": "text"
      },
      "source": [
        "### Data\n",
        "\n",
        "Let's start off with the classic - Shakespeare. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrVfnqWk9aZC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(counter):\n",
        "    s = float(sum(counter.values()))\n",
        "    return [(c, cnt / s) for c, cnt in counter.items()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SKbMyr13Ywk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_char_lm(fname, order=4):\n",
        "    with open(fname, 'r') as f:\n",
        "        data = f.read()\n",
        "\n",
        "        lm = defaultdict(Counter)\n",
        "        pad = PADDING * order\n",
        "        data = pad + data\n",
        "        for i in range(len(data)-order):\n",
        "            history, char = data[i:i+order], data[i+order]\n",
        "            lm[history][char] += 1\n",
        "\n",
        "        outlm = {hist: normalize(chars) for hist, chars in lm.items()}\n",
        "        return outlm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnIU9haN4COK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lm = train_char_lm(\"shakespeare_input.txt\", order=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXPCs2Mb4EzN",
        "colab_type": "text"
      },
      "source": [
        "Let's test the Language Model (lm). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3T91PhRB4EDn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lm['ello']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bS9jSo68mkq5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lm['Firs']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwtMxYmqmm9j",
        "colab_type": "text"
      },
      "source": [
        "What do we learn from this?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg52M_l5mo1b",
        "colab_type": "text"
      },
      "source": [
        "### Generating text\n",
        "Now to the fun part. We want to generate text!\n",
        "\n",
        "To generate text we'll generate one letter (character) at a time. We will look at history and the last order of characters, from this we will sample a letter based on the distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aMZbGW3urg6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_letter(lm, history, order):\n",
        "        history = history[-order:]\n",
        "        dist = lm[history]\n",
        "        x = random()\n",
        "        for c,v in dist:\n",
        "            x -= v # Done to have some more randomization\n",
        "            if x <= 0: return c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFwlbaNnvGZv",
        "colab_type": "text"
      },
      "source": [
        "But generating letters doesn't make a text, we need something that glues this together. We need to generate the text out of the letters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8skv8DtvMcv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(lm, order, nletters=1000):\n",
        "    history = PADDING * order\n",
        "    out = []\n",
        "    for i in range(nletters):\n",
        "        c = generate_letter(lm, history, order)\n",
        "        history = history[-order:] + c\n",
        "        out.append(c)\n",
        "    return \"\".join(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2B4NmpSvcoA",
        "colab_type": "text"
      },
      "source": [
        "### Order = 2\n",
        "Let's give it a try!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J50YpwHgvd9I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lm = train_char_lm(\"shakespeare_input.txt\", order=2)\n",
        "print(generate_text(lm, 2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "warLqEROvicx",
        "colab_type": "text"
      },
      "source": [
        "### Order = 4 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SAXEzIrvkLO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lm = train_char_lm(\"shakespeare_input.txt\", order=4)\n",
        "print(generate_text(lm, 4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqWaMM57voTw",
        "colab_type": "text"
      },
      "source": [
        "### Order = 7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgGHl6kQvp-O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lm = train_char_lm(\"shakespeare_input.txt\", order=7)\n",
        "print(generate_text(lm, 7))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxSiWAdnvr3G",
        "colab_type": "text"
      },
      "source": [
        "### Order = 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18zzb6scvtdf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lm = train_char_lm(\"shakespeare_input.txt\", order=10)\n",
        "print(generate_text(lm, 10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrbxzBRovvT9",
        "colab_type": "text"
      },
      "source": [
        "### Conclusion of step 1\n",
        "We find that at order = 4 we get reasonable results that only get better with the higher order.\n",
        "\n",
        "We can find that this generation does not support out-of-vocabulary, and can only generate history. This is \"so-so\".  \n",
        "With the famous LSTM (RNN) char2char we can generate new texts & with its memory it can remember to open and end paranthesis, to know that an birthdate is connected with a location often and so on. LSTM can remember for a looong time, as we will see in the (probably) next workshop.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99kV9gXJzBO9",
        "colab_type": "text"
      },
      "source": [
        "### Word by Word Text Generation\n",
        "Let's spin this around to instead generate words, will it work?  \n",
        "Of course it will when we're driving the vehicle!  \n",
        "\n",
        "we want learn a function $P(w|h)$. Here, $w$ is a word, $h$ is a n-word history, and $P(w|h)$ stands for how likely is it to see $w$ after we've seen $h$. See earlier explanation for Bigram-model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o40OiFT8OFNh",
        "colab_type": "text"
      },
      "source": [
        "#### Preprocessing (over & over again!)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rk9y5OMnQCuO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(text):\n",
        "  doc = nlp(text,  disable=['parser', 'tagger', 'ner'])\n",
        "  return [str(token) for token in doc]\n",
        "\n",
        "def preprocess(text):\n",
        "  return str(text).lower()\n",
        "  \n",
        "def pandas_preprocess(dataframe):\n",
        "  dataframe = dataframe.applymap(preprocess)\n",
        "  return dataframe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTGWS2xRfdI5",
        "colab_type": "text"
      },
      "source": [
        "#### Training word level generation language model\n",
        "First, just as with character-level, we need to train the model (count words that is).  \n",
        "The same techniques is applied as with character level generation with the difference that we now count words. And we require more data for a good generation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVvzdXpa-gtR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_word(lm, history, order):\n",
        "    history = history[-order:]\n",
        "    history_key = ' '.join(history)\n",
        "    dist = lm[history_key]\n",
        "    x = random()\n",
        "    for c, v in dist:\n",
        "        x = x - v\n",
        "        if x <= 0: return c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtIDmunZ-g8d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text_word(lm, order, nletters=25):\n",
        "    history = [PADDING] * order\n",
        "    out = []\n",
        "    for i in range(nletters):\n",
        "        c = generate_word(lm, history, order)\n",
        "        history = history[-order:] + [c]\n",
        "        out.append(c)\n",
        "    return ' '.join(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVrtVRr2e99E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_word_lm(fname, order=2):\n",
        "    with open(fname, 'r') as f:\n",
        "        data = f.read()\n",
        "        words = tokenize(data)\n",
        "        lm = defaultdict(Counter)\n",
        "        pad = [PADDING] * order\n",
        "        data = pad + words\n",
        "        for i in range(len(data)-order):\n",
        "            history, word = data[i:i+order], data[i+order]\n",
        "            lm[' '.join(history)][word] += 1\n",
        "\n",
        "        outlm = {hist: normalize(words) for hist, words in lm.items()}\n",
        "        return outlm\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsD2ikpQAHes",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "order = 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTciO9O0gDA2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lm = train_word_lm(\"shakespeare_input.txt\", order)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqZ9IjZeACI_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(generate_text_word(lm, order))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGaY2x9BRuXs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Difference to Markov Chain - https://stackoverflow.com/a/24419604\n",
        "# https://blog.dataiku.com/2016/10/08/machine-learning-markov-chains-generate-clinton-trump-quotes\n",
        "# With c2c add <sos> & <eos>, perhaps to word too."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ichgXE_CCWD3",
        "colab_type": "text"
      },
      "source": [
        "## NLTK\n",
        "NLTK is one of the important libraries for someone who works with text. It contains a lot of tooling that can simplify our lives, so let's try to reimplement this using NLTK & their corpuses.\n",
        "\n",
        "Project Gutenberg is a famous corpus containing about 25 000 e-books, including Shakespeare, Jane Austen etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-PIY_5JChtg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "from nltk.util import ngrams\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk import FreqDist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0h0AHlhCvAK",
        "colab_type": "text"
      },
      "source": [
        "When using NLTK-corpuses that are already processed and beautiful we get some bonuses, we can extract sentences, ord or raw.  \n",
        "When taking the sentences it's already tokenized & ready for use which is pretty damn awesome."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjtgARR3DZf9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(gutenberg.sents()[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiSgQSrRDhUc",
        "colab_type": "text"
      },
      "source": [
        "As our text is tokenized, let's start of by using what Gutenberg givs us. We can apply `lowercase` later into the process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Af6YirvMCtWR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_word_lm_nltk(order=3):\n",
        "  gut_ngrams = ( ngram for sent in gutenberg.sents() for ngram in ngrams(sent, order, pad_left = True, pad_right = True, right_pad_symbol='EOS', left_pad_symbol=\"BOS\"))\n",
        "  ngram_prob = defaultdict(Counter)\n",
        "  for ngram in gut_ngrams:\n",
        "      ngram_prob[ngram[0] + ngram[1]][ngram[2]] += 1\n",
        "      # ngram_prob[ngram[0]][ngram[2]] += 1 <-- BackOff\n",
        "  outlm = {hist: normalize(chars) for hist, chars in ngram_prob.items()}\n",
        "  return outlm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kn1YutSLzAi7",
        "colab_type": "text"
      },
      "source": [
        "### Improvements\n",
        "Okey, this is all good. We can now generate text and we can easily swap the data that we use (just a `.txt` file).  \n",
        "The first improvemet we can do is called _smoothing_. \n",
        "\n",
        "#### Smoothing\n",
        "Smoothing does just what the name suggests, we smooth data. In other words, we allow OOV (out-of-vocabulary) words to be used. This is incredibly important and can help us generate much better text.  \n",
        "\n",
        "**Laplacian Smoothing**  \n",
        "Simplest approach, very naÃ¯ve. There's two ways, either _add-one smoothing_ or _add-k smoothing_.\n",
        "\n",
        "\n",
        "**Katz-Backoff**  \n",
        "Longer N-grams are better, but if it doesn't exist back off to a shorter one.\n",
        "\n",
        "**Interpolation Smoothing**  \n",
        "Use multiple N in N-grams to get total prob.\n",
        "\n",
        "**Kneser-Key Smoothing**  \n",
        "Most popular one, but hard to implement correctly.  \n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*pMttoEXAH_GS9d6AtkhF2g.png)  \n",
        "Very good explanation through a [blog](https://medium.com/@seccon/a-simple-numerical-example-for-kneser-ney-smoothing-nlp-4600addf38b8)\n",
        "\n",
        "#### Smoothing by UNK\n",
        "Another approach could be to smooth the most uncommon words by UNK. This could for example be names if they're rare. In that case we would have names more commonly, and as such perhaps generate \"*UNK was a man of honor*\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nj_7ovp2AzBi",
        "colab_type": "text"
      },
      "source": [
        "## Smooth by Backoff & UNK\n",
        "Let's implement smoothing using Backoff & UNK-token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8mAu4jfI9Xt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# BackOff = if < X options for an bigram, choose the unigram prob\n",
        "# UNK = find the least common words, replace them by UNK and redo."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttCYMUBD6mOC",
        "colab_type": "text"
      },
      "source": [
        "### Tips on fun to do at home till next time\n",
        "Markovify: https://github.com/jsvine/markovify\n",
        "\n",
        "This is basically what we've done.\n",
        "\n",
        "### Harder improvements\n",
        "Create a Hidden Markov Model that also makes use of the POS."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDUjwCTRBCIP",
        "colab_type": "text"
      },
      "source": [
        "## What's in store for future sessions?\n",
        "\n",
        "\n",
        "*   Neural Networks (will improve result)\n",
        "*   State-of-the-Art Neural Network using GPT-2 & transfer learning\n",
        "*  Generate something really fun (Trump tweets, Rap songs or whatever we decide)\n",
        "*  Deploying a model\n",
        "*  (If people want too; text generation by selecting texts via Word Embedding & such. Example: Zac_the_second_bot)  \n",
        "\n",
        "\n",
        "\n",
        "If all agree & don't have something else they'd prefer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_i5I89AgLZXt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.util import ngrams\n",
        "from nltk.corpus import gutenberg\n",
        "\n",
        "gut_ngrams = ( ngram for sent in gutenberg.sents() for ngram in ngrams(sent, 3, pad_left = True, pad_right = True, right_pad_symbol='EOS', left_pad_symbol=\"BOS\"))\n",
        "#print(list(gut_ngrams)[:5])\n",
        "freq_dist = nltk.FreqDist(gut_ngrams)\n",
        "print(freq_dist.keys())\n",
        "kneser_ney = nltk.KneserNeyProbDist(freq_dist)\n",
        "\n",
        "prob_sum = 0\n",
        "for i in kneser_ney.samples():\n",
        "    if i[0] == \"Who\" and i[1] == \"are\":\n",
        "        prob_sum += kneser_ney.prob(i)\n",
        "        print(\"{0}:{1}\".format(i, kneser_ney.prob(i)))\n",
        "print(prob_sum)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}