{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Finished] SocialMediaPerception.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONHgehE3O1Kv",
        "colab_type": "text"
      },
      "source": [
        "# Data Mining, Preparation and Understanding\n",
        "Today we'll go through Data Mining, Preparation & Understanding which is a really fun one (and important).  \n",
        "In this notebook we'll try out some important libs to understand & also learn how to parse Twitter with some help from `Twint`. All in all we'll go through `pandas`, `twint` and some more - let's start by installing them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUfrE4zVPhkh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!pip install twint\n",
        "!pip install wordcloud\n",
        "import twint\n",
        "import pandas as pd\n",
        "import tqdm\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RkrROzZJZbG",
        "colab_type": "text"
      },
      "source": [
        "## Tonights theme: ÅF Pöyry (and perhaps some AFRY)\n",
        "To be a Data Miner we need something to mine.\n",
        "![alt text](https://financesmarti.com/wp-content/uploads/2018/09/Dogcoin-Mining-min.jpg)\n",
        "In this case it won't be Doge Coin but rather ÅF, ÅF Pöyry & AFRY.\n",
        "\n",
        "To be honest, it's not the best theme (pretty generic names ones you go ASCII which we'll do to simplify our lifes. \n",
        "\n",
        "### What is Twint\n",
        "`Twint` is a really helpful library to scrape Twitter, it uses the search (i.e. not the API) and simplifies the whole process for us as users.  \n",
        "The other way to do this would be to use either the API yourself (time-consuming to learn and also limited in calls) or to use BS4 (Beatiful Soup) which is a great python-lib to scrape websites. But I'd dare say that it is better for static content sites such as Wikipedia, Aftonbladet etc rather than Twitter etc.  \n",
        "This all together led to the choice of `Twint` _even_ though it has a **huge** disadvantage - it does not support UTF8 from what I can find. \n",
        "\n",
        "\n",
        "### What is pandas\n",
        "Pandas is a library to parse, understand and work with data. It's really fast using the `DataFrame` they supply.  \n",
        "Using this `DataFrame` we can manipulate the data in different ways. It has all the functions you can imagine from both SQL and Excel, a great tool all in all.\n",
        "\n",
        "### Bringing it all together\n",
        "Let's take a look at how we can use this all together!\n",
        "\n",
        "First a quick look at the Twint config."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra12EC4WJPQv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Twint Config:\n",
        "\n",
        "Variable             Type       Description\n",
        "--------------------------------------------\n",
        "Retweets             (bool)   - Display replies to a subject.\n",
        "Search               (string) - Search terms\n",
        "Store_csv            (bool)   - Set to True to write as a csv file.\n",
        "Pandas               (bool)   - Enable Pandas integration.\n",
        "Store_pandas         (bool)   - Save Tweets in a DataFrame (Pandas) file.\n",
        "Get_replies          (bool)   - All replies to the tweet.\n",
        "Lang                 (string) - Compatible language codes: https://github.com/twintproject/twint/wiki/Langauge-codes (sv, fi & en supported)\n",
        "Format               (string) - Custom terminal output formatting.\n",
        "Hide_output          (bool)   - Hide output.\n",
        "\n",
        "Rest of config: https://github.com/twintproject/twint/wiki/Configuration\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCNvFd2MOIR0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c = twint.Config()\n",
        "c.Query\n",
        "c.Search = \" ÅF \"\n",
        "c.Format = \"Username: {username} |  Tweet: {tweet}\"\n",
        "c.Pandas = True\n",
        "c.Store_pandas = True\n",
        "c.Pandas_clean = True\n",
        "c.Show_hashtags = True\n",
        "c.Limit = 10\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_3McxnoOuzl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "twint.run.Search(c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckgRsnTc1BpS",
        "colab_type": "text"
      },
      "source": [
        "**What do we see?**\n",
        "No Swedish, what so ever. This is not interesting for our usecase as all the tweets are about something else really.  \n",
        "Let's try ÅF Pöyry instead"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oapWs9i1HIb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c.Search = \"ÅF AFRY Pöyry\"\n",
        "twint.run.Search(c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mtw1olN1t_d",
        "colab_type": "text"
      },
      "source": [
        "Looking at this we have a much better result. This really shows the power of Ngrams (bigram).  \n",
        "Let's play around some in the next box trying `@AFkarriar` as keyword and also to include `Replies` and some other fields."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrinJfLI2R7J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c.Replies = True\n",
        "twint.run.Search(c)\n",
        "# Play around with params, do whatever!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xweFomgc2ue6",
        "colab_type": "text"
      },
      "source": [
        "### Results\n",
        "Ok, so we have tried out a few different things we can use in `Twint`. For me `@AFkarriar` worked out best - **what was your favorite?**  \n",
        "\n",
        "Let's analyze some more."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XBzNPGL_Ort",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FILENAME = \"afpoyry.csv\"\n",
        "c = twint.Config()\n",
        "c.Query\n",
        "c.Show_hashtags = True\n",
        "c.Search = \"ÅF\"\n",
        "c.Lang = \"sv\"\n",
        "#c.Get_replies = True\n",
        "c.Store_csv = True\n",
        "c.Hide_output = True\n",
        "c.Output = FILENAME\n",
        "twint.run.Search(c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBKrobIBKcoM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(FILENAME)\n",
        "print(data.shape)\n",
        "print(data.dtypes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfCqjskH61B2",
        "colab_type": "text"
      },
      "source": [
        "### Cleaning\n",
        "We can most likely clean some titles from here, just to make it simpler for us"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMzjx61l6kL2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_less = data.filter([\"tweet\", \"username\"])\n",
        "data_less.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jZ5zHzG72H1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_less[\"tweet\"].head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNePVoSYQiPH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "from IPython.display import Image\n",
        "\n",
        "t = '\\n'.join([x.tweet for i, x in data_less.iterrows()])\n",
        "\n",
        "WordCloud().generate(t).to_file('cloud.png')\n",
        "Image('cloud.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58L2Vh6lImNZ",
        "colab_type": "text"
      },
      "source": [
        "**Stop Words** - Anyone remember? Let's remove them!  \n",
        "NLTK is a great toolkit for just about everything in NLP, we can find a list of stopwords for most languages here, including Swedish."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75RFcjUFQFwI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "swe_stop = set(stopwords.words('swedish'))\n",
        "list(swe_stop)[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brwAJqngad1K",
        "colab_type": "text"
      },
      "source": [
        "**Stemming** - Anyone remember? Let's do it!\n",
        "NLTK is _the_ lib to use when you want at least _some_ swedish. But I think I've squeezed all the swedish out of NLTK that I can find right now... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sdNxXhbVr28",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        " \n",
        "stemmer = SnowballStemmer(\"swedish\")\n",
        "stemmer.stem(\"hoppade\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LemuFLV0RIs3",
        "colab_type": "text"
      },
      "source": [
        "**Cleaning** - Anyone remember? Let's do it!  \n",
        "![alt text](https://imgflip.com/s/meme/X-All-The-Y.jpg)  \n",
        "To have a \"better\" word cloud we need to reduce the dimensions and keep more important words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8Du0lZKz5uD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!pip install regex"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4lbIYfCO14J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from string import punctuation\n",
        "import regex as re\n",
        "\n",
        "# bad_words = re.compile(\"https|http|pic|www|och|med|att|åf|pöyry|läs\")\n",
        "http_re = re.compile(\"https?.*?(\\w+)\\.\\w+(\\/\\s)?\")\n",
        "whitespace_re = re.compile(\"\\s+\")\n",
        "punc_set = set(punctuation)\n",
        "\n",
        "def clean_punct(tweet):\n",
        "  return ''.join([c for c in tweet if c not in punc_set])\n",
        "\n",
        "def remove_stopwords(tweet):\n",
        "   return \" \".join([t for t in tweet.split(\" \") if t not in swe_stop])\n",
        "\n",
        "# Example of cleaning: remove punct, lowercase, https and stemming/lemmatizing\n",
        "# (we want to reduce the space/dimensions)\n",
        "def clean_text(tweet):\n",
        "  tweet = tweet.lower()\n",
        "  tweet = ' '.join([word for word in tweet.split() if not word.startswith('pic.')])\n",
        "  tweet = http_re.sub(r'\\1', tweet)\n",
        "  tweet = tweet.lower()\n",
        "  tweet = remove_stopwords(clean_punct(tweet)).strip()\n",
        "  tweet = whitespace_re.sub(' ', tweet)\n",
        "  return tweet\n",
        "\n",
        "clean_text(\"hej där borta. hur mår du? vem vet.. Jag vet  inte. http:/google.com pic.twitterlol\")\n",
        "#data_less[\"tweet\"] = data_less[\"tweet\"].apply(lambda x: clean_text(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EK977vs9nt96",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_less[\"tweet\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwMHC2969S6u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "from IPython.display import Image\n",
        "\n",
        "t = '\\n'.join([x.tweet for i, x in data_less.iterrows()])\n",
        "\n",
        "WordCloud().generate(t).to_file('cloud_clean.png')\n",
        "Image('cloud_clean.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5L6_08XOemv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "def print_most_common(wcount, n=5):\n",
        "  for (name, count) in wcount.most_common(n):\n",
        "    print(f\"{name}: {count}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWraaOO2HtSW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t_hash = ' '.join([x for x in t.split() if x.startswith(\"#\")])\n",
        "hash_count = Counter(t_hash.split())\n",
        "WordCloud().generate(t_hash).to_file('cloud_#.png')\n",
        "\n",
        "print_most_common(hash_count, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_sNB6b7I9Rw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t_at = ' '.join([x for x in t.split() if x.startswith(\"@\")])\n",
        "at_count = Counter(t_at.split())\n",
        "WordCloud().generate(t_at).to_file('cloud_@.png')\n",
        "\n",
        "print_most_common(at_count, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5nbjuGsSE7m",
        "colab_type": "text"
      },
      "source": [
        "### WordClouds!\n",
        "Let's take a look at what we've got."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJrpq1okUzbd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Image('cloud_clean.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFvEDso3VImM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Image('cloud_no_stop.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-E7P-hCVGuA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Image('cloud_@.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NvnrXOWVHtZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Image('cloud_#.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_5OS78LSLPs",
        "colab_type": "text"
      },
      "source": [
        "### What to do?\n",
        "A big problem with Swedish is that there's very few models which we can do some fun with, and our time is very limited.  \n",
        "Further on we can do the following:\n",
        "\n",
        "\n",
        "1.   Look at Ngram see if we can see common patterns\n",
        "2.   ...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmnWxjkOzPbU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "1. Try perhaps some type of Ngrams\n",
        "4. Find different shit\n",
        "4. Try to find connections\n",
        "5. Move over to spark (?)\n",
        "https://towardsdatascience.com/nlp-for-beginners-cleaning-preprocessing-text-data-ae8e306bef0f\n",
        "https://medium.com/@kennycontreras/natural-language-processing-using-spark-and-pandas-f60a5eb1cfc6\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaY2rvYQIS8h",
        "colab_type": "text"
      },
      "source": [
        "### AFRY\n",
        "Let's create a wordcloud & everything for AFRY. This is for you to implement fully!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izuCW2rnIUU8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FILENAME2 = \"afry.csv\"\n",
        "c = twint.Config()\n",
        "c.Query\n",
        "c.Show_hashtags = True\n",
        "c.Search = \"afry\"\n",
        "c.Lang = \"sv\"\n",
        "c.Get_replies = True\n",
        "c.Store_csv = True\n",
        "c.Hide_output = True\n",
        "c.Output = FILENAME2\n",
        "twint.run.Search(c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clC9xzByIZqI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_afry = pd.read_csv(FILENAME2)\n",
        "t_afry = '\\n'.join([x.tweet for i, x in data_afry.iterrows()])\n",
        "WordCloud().generate(t_afry).to_file('cloud_afry.png')\n",
        "\n",
        "Image('cloud_afry.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XinMREgXXHOw",
        "colab_type": "text"
      },
      "source": [
        "### Jonas Sjöstedt (jsjostedt) vs Jimmy Åkesson (jimmieakesson)\n",
        "Implementation as follows:\n",
        "1. Get data for both (tip: use `c.Username` or `c.User_id` and don't forget formatting output in terminal if used)\n",
        "2. Clean data\n",
        "3. ?? (Perhaps wordclouds etc)\n",
        "4. TfIdf\n",
        "5. Join ds & shuffle, train clf\n",
        "6. Testing!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH3UmO532oOz",
        "colab_type": "text"
      },
      "source": [
        "## Jimmie Åkesson"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ebuGCHe2nIt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FILENAME = \"jimmie2.csv\"\n",
        "c = twint.Config()\n",
        "c.Query\n",
        "c.Show_hashtags = True\n",
        "#c.Search = \"ÅF\"\n",
        "c.Username = \"jimmieakesson\"\n",
        "#c.Get_replies = True\n",
        "c.Store_csv = True\n",
        "c.Output = FILENAME\n",
        "twint.run.Search(c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FwKjhSB27HI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_jimmie = pd.read_csv(FILENAME)\n",
        "print(data_jimmie.shape)\n",
        "\n",
        "data_less_jimmie = data_jimmie.filter([\"tweet\", \"username\"])\n",
        "data_less_jimmie.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcEHQ-Sw3Zbr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_less_jimmie[\"tweet\"] = data_less_jimmie[\"tweet\"].apply(lambda x: clean_text(x))\n",
        "data_less_jimmie.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZO5spBcl4Gws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "from IPython.display import Image\n",
        "\n",
        "t = '\\n'.join([x.tweet for i, x in data_less_jimmie.iterrows()])\n",
        "\n",
        "WordCloud().generate(t).to_file('cloud_clean_jimmie.png')\n",
        "Image('cloud_clean_jimmie.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuvLqFqsAbCD",
        "colab_type": "text"
      },
      "source": [
        "## Jonas Sjöstedt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JP3scL4AZeT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FILENAME_J = \"jonas.csv\"\n",
        "c = twint.Config()\n",
        "c.Query\n",
        "c.Show_hashtags = True\n",
        "#c.Search = \"ÅF\"\n",
        "c.Username = \"jsjostedt\"\n",
        "#c.Get_replies = True\n",
        "c.Store_csv = True\n",
        "c.Hide_output = True\n",
        "c.Output = FILENAME_J\n",
        "twint.run.Search(c)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TG5a1FvRBHWz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_jonas = pd.read_csv(FILENAME_J)\n",
        "print(data_jonas.shape)\n",
        "\n",
        "data_less_jonas = data_jonas.filter([\"tweet\", \"username\"])\n",
        "data_less_jonas.head()\n",
        "\n",
        "data_less_jonas[\"tweet\"] = data_less_jonas[\"tweet\"].apply(lambda x: clean_text(x))\n",
        "data_less_jonas.head()\n",
        "\n",
        "t = '\\n'.join([x.tweet for i, x in data_less_jonas.iterrows()])\n",
        "\n",
        "WordCloud().generate(t).to_file('cloud_clean_jonas.png')\n",
        "Image('cloud_clean_jonas.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WY7ygv_QBrgg",
        "colab_type": "text"
      },
      "source": [
        "# TfIdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mltkGLTOBu0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKesBp7wCGIV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cv=TfidfVectorizer(ngram_range=(1,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAgA1YoQDX5m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_count_vector_jonas = cv.fit_transform(data_less_jonas[\"tweet\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwUvWT9BDxCd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_names = cv.get_feature_names()\n",
        "\n",
        "#get tfidf vector for first document\n",
        "first_document_vector=word_count_vector_jonas[0]\n",
        " \n",
        "#print the scores\n",
        "df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"])\n",
        "df.sort_values(by=[\"tfidf\"],ascending=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SARY4EyZDePJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_count_vector_jimmie = cv.fit_transform(data_less_jimmie[\"tweet\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndczOwE7GML9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_names = cv.get_feature_names()\n",
        "\n",
        "#get tfidf vector for first document\n",
        "first_document_vector=word_count_vector_jimmie[2]\n",
        " \n",
        "#print the scores\n",
        "df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"])\n",
        "df.sort_values(by=[\"tfidf\"],ascending=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrP6MT8vBvOH",
        "colab_type": "text"
      },
      "source": [
        "# Join dfs & shuffle, train clf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57rzUwsr1N0Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(data_jimmie.shape)\n",
        "print(data_jonas.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pl_fRJBtB1Ed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "tfidf = TfidfVectorizer(ngram_range=(1,2))\n",
        "\n",
        "data_less_jonas = data_less_jonas.head(2581)\n",
        "print(data_less_jonas.shape)\n",
        "\n",
        "combined = pd.concat([data_less_jimmie,data_less_jonas])\n",
        "combined = shuffle(combined)\n",
        "print(combined.shape)\n",
        "combined.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqBQ4owq1tCh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "tweet_tfidf = tfidf.fit_transform(combined[\"tweet\"])\n",
        "X_train, X_test, y_train, y_test = train_test_split(tweet_tfidf, combined[\"username\"], test_size=0.1, random_state=42)\n",
        "X_train[:3]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZJCv9t6JtCQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "clf = LinearSVC()\n",
        "\n",
        "model = clf.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nH1bwSC3IVR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "y_pred = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrnVo9l2B1iq",
        "colab_type": "text"
      },
      "source": [
        "# Testing!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yB4TFwvDB3fk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def testClassifier(tweet):\n",
        "  vector = tfidf.transform([clean_text(tweet)])\n",
        "\n",
        "  print(model.predict(vector))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqADTgpFKunY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testClassifier(\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFshP4qpK2yI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testClassifier(\"Arbetsmarknaden är inte fri svenska kollektivavtal privatisering arbetslösa kommun\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uql88E0cjrkj",
        "colab_type": "text"
      },
      "source": [
        "# Going forward\n",
        "I see 4 options:\n",
        "\n",
        "\n",
        "1.   Find stuffs that can help people in the office (@AFRY)\n",
        "2.   Create models for Swedish and perhaps Open Source\n",
        "3.   Make \"interesting\"/\"fun\" stuffs (such as applying Text Generation on something like Cards Against Humanity etc)\n",
        "4.   Try something new (perhaps Image Recognition?)\n",
        "\n",
        "Focusing on Swedish is only possible in 1 & 2.\n",
        "\n",
        "Some concrete options:\n",
        "* Explore SparkNLP\n",
        "* Ask around at AFRY for things to automate\n",
        "* Apply text-generation with SOTA to generate either something like Cards Against Humanity or some persons Tweet etc.\n",
        "* Create datasets to create Swedish models on (might need a mech-turk; this will be pretty time-consuming before we see any type of results).\n",
        "* Something completely different.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWQJi9rzByzH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOz3i6k4BzL5",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}