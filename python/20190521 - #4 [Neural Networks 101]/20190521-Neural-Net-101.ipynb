{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NeuralNetworksAndLayers.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"2Dqg0v117nUe","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"RnfAifDwX3Nj","colab_type":"text"},"cell_type":"markdown","source":["## Neural Networks: Network, Layers & Neurons\n","Different architectures & neurons are possible to be used. In one way it's a very advanced LEGO, you just need to understand what each LEGO part is built of and how it interacts with everything else and how the LEGO works as a system all together. \n","![Neural Network](https://pvsmt99345.i.lithium.com/t5/image/serverpage/image-id/42339i8BA3F2CCCEDE7458/image-size/large?v=1.0&px=999)\n","\n","A Layer in a Neural Network is built from multiple Neurons being stacked in width. \n","I'll start from Neurons and build it upwards to first understand the single LEGO piece and then the bigger picture. \n","\n","### Backpropagation\n","Backpropagation is the key in neural network, it is used to calculate the _gradient_ that is required by the \"update\" function in the neuron where it recalculates its weights. Backpropagation is short for \"the backward propagation of errors\".   \n","In simple terms, we push backward what we learned by our prediction and how well it went compared to the answer and we update our network automatically to create a better guess next time.\n"]},{"metadata":{"id":"o-bdGZyqX3tC","colab_type":"code","colab":{}},"cell_type":"code","source":["# PyTorch\n","# Fully connected neural network with one hidden layer\n","class NeuralNet(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_classes):\n","        super(NeuralNet, self).__init__()\n","        self.fc1 = nn.Linear(input_size, hidden_size) \n","        self.relu = nn.ReLU()\n","        self.fc2 = nn.Linear(hidden_size, num_classes)  \n","    \n","    def forward(self, x):\n","        out = self.fc1(x)\n","        out = self.relu(out)\n","        out = self.fc2(out)\n","        return out\n","# Init model\n","model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n","\n","# Train the model\n","total_step = len(train_loader)\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):  \n","        # Move tensors to the configured device\n","        images = images.reshape(-1, 28*28).to(device)\n","        labels = labels.to(device)\n","        \n","        # Forward pass\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        \n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","        if (i+1) % 100 == 0:\n","            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n","                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n","\n","# Test the model\n","# In test phase, we don't need to compute gradients (for memory efficiency)\n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for images, labels in test_loader:\n","        images = images.reshape(-1, 28*28).to(device)\n","        labels = labels.to(device)\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n","\n","# Save the model checkpoint\n","torch.save(model.state_dict(), 'model.ckpt')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tYDcr9QZ1-0H","colab_type":"code","colab":{}},"cell_type":"code","source":["# Keras\n","\n","# create model\n","model = Sequential()\n","model.add(Dense(12, input_dim=8, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile model\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Fit the model\n","model.fit(X, Y, epochs=150, batch_size=10)\n","\n","# evaluate the model\n","scores = model.evaluate(X, Y)\n","print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n","\n","# calculate predictions\n","predictions = model.predict(X)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RWrcGZtn9sr3","colab_type":"text"},"cell_type":"markdown","source":["### Neurons\n","There exist an crazy amont of neurons that can be used and I'll try to quickly go through the most common ones. \n","![Neuron](https://cdn-images-1.medium.com/max/1200/1*SJPacPhP4KDEB1AdhOFy_Q.png)\n","\n","#### 1. Dense / Linear\n","Dense layers are the most simple. They're basically just an matrix (dot) multiplication. Each neuron receives input from all the neurons in the earlier layer, hence densely connected.  \n","In Keras it's implemented as the following: `output = activation(dot(input, kernel) + bias)`  \n","We also include a bias vector as well as the weight vector on this. The activation is selected when creating the layer.\n","\n","#### 2. Dropout\n","Dropout is a way to combat overfitting. We randomly drop a node completely from the network, this is done by supplying a float inbetween 0 and 1.  \n","There is another dropout called **Spatial Dropout**. If we think about CNNs this would rather remove the whole feature map (convolution) rather than individual pixels. So basically we drop an entire featuremap instead of an individual feature.\n","![dropout gif](http://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/04/1IrdJ5PghD9YoOyVAQ73MJw.gif)\n","\n","#### 3. Pooling\n","Pooling is used to reduce dimensions, by doing this we also make the network more independent in form of input / pattern transition / changes. It goes very well hand in hand with CNNs (which will be explained soon). We have a few different types of pooling such as Maxpool, Avgpool and Global XPool.\n","![pooling layer gif](https://cdn-images-1.medium.com/max/1200/1*ZCjPUFrB6eHPRi4eyP6aaA.gif)\n","\n","#### 4. Recurrent: LSTM & GRU\n","The \"two\" neurons that exist:  \n","LSTM - Long Short Term Memory.  \n","GRU - Gated Recurrent Unit  \n","\n","ELI5: We add a memory to the gate. This makes the network context aware which is great for natural language. We can now remember things over a sequence\n","\n","More in depth we have three outputs of an LSTM and two out of an GRU. We have the cell-state (c), hidden state (h) & for LSTM we also have output (o). Important to add on this is that outside of memory we also have the possibility to forget.   \n","LSTM & GRU solves the problem of vanishing gradient where when we backpropagation.  \n","As they're very complex I won't go through the teory but just the basics.\n","\n","I might extend this.\n","\n","### Layers/Network\n","\n","All from above are layers too. \n","\n","#### 1. Convolutional\n","\n","One can see Convolutional Neural Networks (CNNs) as a type of filter. We apply matrix multiplication to find some kind of feature. One example of this is in images one such matrix multiplication could be to find all edges in an image.  \n","In CNNs, as with other Neural Networks, this is trained automatically by backpropagation. \n","![alt text](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/28132045/cnnimage.png)\n","\n","#### 2. Embedding\n","\n","The embedding layer does exactly what the name suggests, embedds the feature. In our case we always talk about Word Embeddings, and I'm not sure there is any other use-case of embedding layers. \n","\n","How we did it:\n","![alt text](https://cdn-images-1.medium.com/max/1200/1*YEJf9BQQh0ma1ECs6x_7yQ.png)\n","\n","How Embedding does it (dense representation):\n","![alt text](https://cdn-images-1.medium.com/max/1200/0*mRGKYujQkI7PcMDE.)\n","\n","\n","#### 3. Recurrent: LSTM & GRU\n","![RNN](https://cdn-images-1.medium.com/max/800/1*XosBFfduA1cZB340SSL1hg.png)\n"," \n","![RNN_unfolded](http://www.wildml.com/wp-content/uploads/2015/09/rnn.jpg)\n","\n","\n","#### 4. Seq2Seq\n","Sequence 2 sequence is an special version RNNs. \n","\n","\n","#### Bonus: Capsule, Autoencoding & Attention\n","\n","Let's **CODE**!"]},{"metadata":{"id":"F2uc5fJBL52D","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}